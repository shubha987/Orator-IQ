{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Extracting Audio from Video\n",
        "import subprocess\n",
        "\n",
        "def extract_audio(video_file, output_audio_file):\n",
        "    command = f\"ffmpeg -i {video_file} -q:a 0 -map a {output_audio_file} -y\"\n",
        "    subprocess.run(command, shell=True, check=True)\n",
        "\n",
        "# Example usage\n",
        "video_file = \"HR.mp4\"\n",
        "audio_file = \"response_audio.wav\"\n",
        "extract_audio(video_file, audio_file)\n"
      ],
      "metadata": {
        "id": "ae-TdGSdj50e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Noise Reduction\n",
        "import noisereduce as nr\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "# Load audio\n",
        "audio, sr = librosa.load(\"response_audio.wav\", sr=16000)\n",
        "\n",
        "# Apply noise reduction\n",
        "reduced_noise = nr.reduce_noise(y=audio, sr=sr)\n",
        "\n",
        "# Save processed audio\n",
        "sf.write(\"processed_audio.wav\", reduced_noise, sr)\n"
      ],
      "metadata": {
        "id": "1AFKuPcWj8Zn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio to Text\n",
        "import whisper\n",
        "import wave\n",
        "\n",
        "def real_time_transcription(audio_file):\n",
        "    model = whisper.load_model(\"base\")\n",
        "    audio = whisper.load_audio(audio_file)\n",
        "    transcription = model.transcribe(audio)\n",
        "    print(\"Transcription:\", transcription[\"text\"])\n",
        "    return transcription[\"text\"]\n",
        "\n",
        "# Usage\n",
        "# Save live audio to 'audio.wav' in real time\n",
        "transcription = real_time_transcription(\"processed_audio.wav\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpMHjpxiie0X",
        "outputId": "aa554293-4ef6-4b14-d8a3-a91d6377b9d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:02<00:00, 66.0MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:109: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription:  Have you received? Yes, my name is Rizim. What you could make? My name is Rizim Rekres from it. So, Rizim, today you are going to come take a walk into the hall or an extra right desk. So, 10 month ago, yes, firstly, and then to give you a 30-minute service, to introduce this writer, my name is Rizim Rekres from the time from the World Call of Studies and Data Fundamentals. I completed my data in the stream of computer science and engineering, Drakash Mejiri in the college at Harvuku, Bangladesh. I set up my data with 7.96 EPA. Coming to my family, there are proud members of my family, including me and my technical skills and good-dips, me, fighter, data switches and basics of the school. And my interpersonal skills, I am able to communicate with other people's and good listeners and my hobbies are spending time with family and friends. Cooking, business, music and crafting at three times. My strengths are I am a Holger and a self-motivator. And coming to my booth, my short-term goal is to be in a position where I place my heart and my homecoming is to become a successful person and helps to a Riz. But should I hire you? I hope I should work and I am to plan my new partner and I am always ready to do Holger to complete the task on time. And I think this platform and this position will help me to gain these skills and knowledge and achieve my goals. So, Shami, I am here with what is a different between smarter and harder, smart and different between harder and smarter. Hard work involves putting a significant amount of efforts and time to accomplish it as smarter, even more efficient and effective ways to finally solution for the same task with less efforts. But with the both combination, we will get the success. Are you ready to relocate? Yes, I would like to relocate. I am always ready to move to other places and explore the places. And I am always happy to work with the new people with different culture. It used to explain to me. Then you want to see yourself in next two or five years. In next two or five years, I want to be a good position in the competition. That is technically with the team in company, we will have a place and we will plan my career accordingly. What is important for you in your life? For Alkal, money. Yes, man. Mostly I think it worked more than the money because if you work perfectly, you will gain the knowledge and skills and experiences. It does not mean my money is not important. Money is also important to me, but not more than the work. Money gives financial support and security, but we will use our satisfaction and knowledge. If you have work knowledge, you automatically will get the money. For that, you salary expectation. As a pressure, my first priority is to enhance my skills, knowledge and experience. Coming to this service, I will go according to the company's forms. I will expect considerable salary for this position. It will help you for the economic needs. Modern awareness, I get to do details in the work. And spending more time in the small business, but now I have been working on it. I am working on the skills and you work on it. How do you rate yourself on a scale of 1 to 10? I would like to rate myself. I serve as one point in time. I know that I am not perfect. I think I always keep that. There is always scope for lending and improvement. My thinking is that the continuous lending is fundamental to personal and fundamental part to personal and professional work. If you like for the company, I try to find solution for the problem. If I write down later the company will face the problem because of me. It is not a good firm company and not for me. For every problem, there is a solution. I try to resolve this problem. It will help you. If situation demands, it is a perfect growth of the company. I will try. Okay, so we have done with the rate. I will get back to you in a minute. It is okay. I will get back to you in a minute.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grammar Analysis\n",
        "import language_tool_python\n",
        "\n",
        "def grammar_analysis(text):\n",
        "    tool = language_tool_python.LanguageTool('en-IN')\n",
        "    matches = tool.check(text)\n",
        "    errors = [match.ruleId for match in matches]\n",
        "    suggestions = [match.message for match in matches]\n",
        "    return errors, suggestions\n",
        "\n",
        "# Example Usage\n",
        "text = \"This is an test sentence with error.\"\n",
        "errors, suggestions = grammar_analysis(text)\n",
        "print(\"Errors:\", errors)\n",
        "print(\"Suggestions:\", suggestions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwCUNeXDkvXl",
        "outputId": "f126f157-ed90-4765-a073-75d210f2ca5a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading LanguageTool 6.4: 100%|██████████| 246M/246M [00:03<00:00, 69.9MB/s]\n",
            "INFO:language_tool_python.download_lt:Unzipping /tmp/tmp74hsgobm.zip to /root/.cache/language_tool_python.\n",
            "INFO:language_tool_python.download_lt:Downloaded https://www.languagetool.org/download/LanguageTool-6.4.zip to /root/.cache/language_tool_python.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errors: ['EN_A_VS_AN']\n",
            "Suggestions: ['Use “a” instead of ‘an’ if the following word doesn’t start with a vowel sound, e.g. ‘a sentence’, ‘a university’.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pronunciation Analysis\n",
        "from nltk.corpus import cmudict\n",
        "\n",
        "def check_pronunciation(word):\n",
        "    pronunciation = cmudict.dict()\n",
        "    return pronunciation.get(word.lower(), \"Word not found in dictionary\")\n",
        "\n",
        "# Example Usage\n",
        "word = \"hello\"\n",
        "result = check_pronunciation(word)\n",
        "print(f\"Pronunciation for '{word}': {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8pl4-B_kyj8",
        "outputId": "9b60490b-e25b-4745-8a1c-1a105d28d0e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pronunciation for 'hello': [['HH', 'AH0', 'L', 'OW1'], ['HH', 'EH0', 'L', 'OW1']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Speaking Rate Calculation\n",
        "from pydub import AudioSegment, silence\n",
        "\n",
        "def speaking_rate_from_speaking_segments(transcribed_text, audio_file):\n",
        "    \"\"\"\n",
        "    Calculate speaking rate based on automatically detected speaking segments.\n",
        "\n",
        "    :param transcribed_text: Transcribed text of the candidate's speech.\n",
        "    :param audio_file: Path to the audio file.\n",
        "    :return: Speaking rate (WPM) and total speaking time (seconds).\n",
        "    \"\"\"\n",
        "    audio = AudioSegment.from_wav(audio_file)\n",
        "\n",
        "    # Detect non-silent segments (candidate speaking)\n",
        "    non_silent_ranges = silence.detect_nonsilent(audio, min_silence_len=700, silence_thresh=-40)\n",
        "\n",
        "    # Calculate total speaking time\n",
        "    total_speaking_duration = sum((end - start) for start, end in non_silent_ranges) / 1000.0  # in seconds\n",
        "\n",
        "    # Calculate speaking rate (WPM)\n",
        "    words = len(transcribed_text.split())\n",
        "    if total_speaking_duration > 0:\n",
        "        speaking_rate = words / (total_speaking_duration / 60)  # Words per minute\n",
        "    else:\n",
        "        speaking_rate = 0  # Handle edge case where no speaking is detected\n",
        "\n",
        "    return speaking_rate, total_speaking_duration\n",
        "\n",
        "# Example Usage\n",
        "transcription = \"This is a sample response provided by the candidate.\"\n",
        "rate, speaking_time = speaking_rate_from_speaking_segments(transcription, \"processed_audio.wav\")\n",
        "print(f\"Speaking Rate: {rate:.2f} WPM, Total Speaking Time: {speaking_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUAw8rqLk8Fn",
        "outputId": "a9874cd1-4027-4c1b-c630-7fa260b2300e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaking Rate: 1843.00 WPM, Total Speaking Time: 0.29 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pause Analysis\n",
        "from pydub import AudioSegment, silence\n",
        "\n",
        "def pause_analysis_during_speech(audio_file, transcription, min_silence_len=500, silence_thresh=-40):\n",
        "    \"\"\"\n",
        "    Analyze pauses during candidate speaking time, excluding question-reading time.\n",
        "\n",
        "    :param audio_file: Path to the audio file.\n",
        "    :param transcription: Transcribed text of the entire audio.\n",
        "    :param min_silence_len: Minimum length of silence to be considered a pause (in ms).\n",
        "    :param silence_thresh: Silence threshold in dBFS.\n",
        "    :return: Number of pauses, details of pause durations (start, end).\n",
        "    \"\"\"\n",
        "    audio = AudioSegment.from_wav(audio_file)\n",
        "\n",
        "    # Detect speaking segments (non-silent parts)\n",
        "    non_silent_ranges = silence.detect_nonsilent(audio, min_silence_len=700, silence_thresh=silence_thresh)\n",
        "\n",
        "    # Extract candidate's speaking segments (skip question-reading time)\n",
        "    # Assuming the transcription helps identify when the candidate starts speaking\n",
        "    speaking_segments = non_silent_ranges  # Adjust this if you have markers for when speaking starts\n",
        "\n",
        "    # Analyze pauses within speaking segments\n",
        "    total_pauses = 0\n",
        "    pause_details = []\n",
        "\n",
        "    for start, end in speaking_segments:\n",
        "        # Extract the candidate's speaking segment\n",
        "        segment_audio = audio[start:end]\n",
        "\n",
        "        # Detect pauses within this segment\n",
        "        pauses = silence.detect_silence(segment_audio, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n",
        "\n",
        "        # Adjust pause times relative to the original audio\n",
        "        pauses = [(start + pause_start, start + pause_end) for pause_start, pause_end in pauses]\n",
        "\n",
        "        # Update total pauses and details\n",
        "        total_pauses += len(pauses)\n",
        "        pause_details.extend(pauses)\n",
        "\n",
        "    return total_pauses, pause_details\n",
        "\n",
        "# Example Usage\n",
        "audio_file = \"processed_audio.wav\"\n",
        "transcription = \"The transcription of the candidate's entire speech.\"\n",
        "num_pauses, pause_details = pause_analysis_during_speech(audio_file, transcription)\n",
        "print(f\"Number of Pauses During Candidate Speaking: {num_pauses}\")\n",
        "print(\"Pause Durations:\", pause_details)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGuHvZDlk-N3",
        "outputId": "feed8eac-7d2c-46ef-fc00-f874835cb2f5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Pauses During Candidate Speaking: 0\n",
            "Pause Durations: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filler Word Usage Count\n",
        "def filler_word_usage(text):\n",
        "    fillers = [\"um\", \"uh\", \"like\", \"you know\", \"sort of\"]\n",
        "    filler_count = {filler: text.lower().count(filler) for filler in fillers}\n",
        "    return filler_count\n",
        "\n",
        "# Example Usage\n",
        "text = \"Um, I think this is, like, a sort of test.\"\n",
        "fillers = filler_word_usage(text)\n",
        "print(\"Filler Word Usage:\", fillers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHqqPLSqlBJG",
        "outputId": "0bf0f2b1-0a54-4630-e507-1a686e3a0016"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filler Word Usage: {'um': 1, 'uh': 0, 'like': 1, 'you know': 0, 'sort of': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Voice Clarity Analysis\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "\n",
        "def calculate_snr(audio_file):\n",
        "    audio = AudioSegment.from_file(audio_file)\n",
        "    samples = np.array(audio.get_array_of_samples())\n",
        "    signal = np.mean(samples**2)\n",
        "    noise = np.var(samples)\n",
        "    snr = 10 * np.log10(signal / noise)\n",
        "    return snr\n",
        "\n",
        "# Example Usage\n",
        "snr = calculate_snr(\"processed_audio.wav\")\n",
        "print(f\"Signal-to-Noise Ratio: {snr} dB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5izPD_EJlEMb",
        "outputId": "86b17376-a671-401e-fe78-abf8a84f9ded"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Signal-to-Noise Ratio: -9.984965081863532 dB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feedback Generation\n",
        "from pydub import AudioSegment, silence\n",
        "\n",
        "def generate_feedback(transcription, audio_file):\n",
        "    \"\"\"\n",
        "    Generate comprehensive feedback for a candidate's speech.\n",
        "\n",
        "    :param transcription: Transcribed text of the candidate's speech.\n",
        "    :param audio_file: Path to the processed audio file.\n",
        "    :return: Dictionary containing detailed feedback.\n",
        "    \"\"\"\n",
        "    # Grammar Analysis\n",
        "    errors, suggestions = grammar_analysis(transcription)\n",
        "\n",
        "    # Speaking Rate and Speaking Time\n",
        "    speaking_rate, speaking_time = speaking_rate_from_speaking_segments(transcription, audio_file)\n",
        "\n",
        "    # Pause Patterns (pauses while speaking)\n",
        "    num_pauses, pauses = pause_analysis_during_speech(audio_file, transcription)\n",
        "\n",
        "    # Filler Word Usage\n",
        "    fillers = filler_word_usage(transcription)\n",
        "\n",
        "    # Voice Clarity\n",
        "    snr = calculate_snr(audio_file)\n",
        "    if snr < 10:\n",
        "        voice_clarity = \"Low\"\n",
        "    elif 10 <= snr < 20:\n",
        "        voice_clarity = \"Medium\"\n",
        "    else:\n",
        "        voice_clarity = \"High\"\n",
        "\n",
        "    # Feedback Compilation\n",
        "    feedback = {\n",
        "        \"Grammar Errors\": errors,\n",
        "        \"Grammar Suggestions\": suggestions,\n",
        "        \"Speaking Rate (WPM)\": speaking_rate,\n",
        "        \"Total Speaking Time (seconds)\": speaking_time,\n",
        "        \"Number of Pauses\": num_pauses,\n",
        "        \"Pause Details (ms)\": pauses,\n",
        "        \"Filler Word Usage\": fillers,\n",
        "        \"Voice Clarity\": voice_clarity,\n",
        "    }\n",
        "    return feedback\n",
        "\n",
        "# Example Usage\n",
        "transcription = \"This is um a sample uh response with errors.\"\n",
        "feedback = generate_feedback(transcription, \"processed_audio.wav\")\n",
        "for key, value in feedback.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFjvKnJrlHkf",
        "outputId": "314e4b7f-7cb8-41a0-bc2f-345eb677a4db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grammar Errors: []\n",
            "Grammar Suggestions: []\n",
            "Speaking Rate (WPM): 1843.0034129692833\n",
            "Total Speaking Time (seconds): 0.293\n",
            "Number of Pauses: 0\n",
            "Pause Details (ms): []\n",
            "Filler Word Usage: {'um': 1, 'uh': 1, 'like': 0, 'you know': 0, 'sort of': 0}\n",
            "Voice Clarity: Low\n"
          ]
        }
      ]
    }
  ]
}